syntax = "proto3";

option java_multiple_files = true;
option java_package = "marmot.proto.service";

package marmot.proto.service;

import public "marmot.proto";
import public "marmot.optor.proto";
import public "marmot.service.proto";
import public "marmot.service.exec.proto";

service DataSetService {
	rpc createDataSet(CreateDataSetRequest) returns (DataSetInfoResponse);
	rpc bindDataSet(BindDataSetRequest) returns (DataSetInfoResponse);
	rpc readDataSet(marmot.proto.StringProto) returns (RecordSetRefResponse);
	rpc queryRange(QueryRangeRequest) returns (RecordSetRefResponse);
	rpc appendRecordSet(AppendRecordSetRequest) returns (LongResponse);
	rpc deleteDataSet(marmot.proto.StringProto) returns (BoolResponse);
	rpc moveDataSet(MoveDataSetRequest) returns (VoidResponse);
	rpc clusterDataSet(ClusterDataSetRequest) returns (SpatialIndexInfoResponse);
	
	rpc getDataSetInfoAll(marmot.proto.VoidProto) returns (stream DataSetInfoResponse);
	rpc getDataSetInfo(marmot.proto.StringProto) returns (DataSetInfoResponse);
	rpc getDataSetInfoAllInDir(DirectoryTraverseRequest) returns (stream DataSetInfoResponse);
	
	//
	// DataSet Directory related interface
	//
	rpc getDirAll(VoidProto) returns (stream StringResponse);
	rpc getSubDirAll(DirectoryTraverseRequest) returns (stream StringResponse);
	rpc getParentDir(StringProto) returns (StringResponse);
	rpc moveDir(MoveDirRequest) returns (VoidResponse);
	rpc deleteDir(StringProto) returns (VoidResponse);
	
	//
	// Visualization interface
	//
	// 주어진 사각 영역과 겹치는 공간 클러스터 (인덱스 블록)들의 등록정보를 반환
	rpc querySpatialClusterInfo(QuerySpatialClusterInfoRequest)
		returns (stream SpatialClusterInfoResponse);
	// 주어진 식별자에 해당하는 공간 클러스터에 포함된 레코드들을 반환
	rpc readSpatialCluster(ReadSpatialClusterRequest) returns (RecordSetRefResponse);
	rpc sampleSpatialCluster(SampleSpatialClusterRequest) returns (RecordSetRefResponse);
	
	// Misc interface
	rpc getBlockSize(StringProto) returns (LongResponse);
	rpc getDataSetLength(marmot.proto.StringProto) returns (LongResponse);
	
	rpc createKafkaTopic(CreateKafkaTopicRequest) returns (VoidResponse);
}

enum DataSetTypeProto {
	FILE = 0;
	LINK = 1;
	TEXT = 2;
	CLUSTER = 3;
}

message DataSetInfoProto {
	message DataSetGeometryInfoProto {
		string column = 1;		// 공간 컬럼 이름
		int32 column_index = 2;	// 공간 컬럼 순번
		string srid = 3;		// 공간 컬럼의 좌표계
		marmot.proto.EnvelopeProto bounds = 4;	// 데이터세트내 모든 컬럼 값의 MBR
	}
	
	string id = 1;				// 데이터세트 식별자
	string dir_name = 2;		// 데이터세트 소속 디렉토리
	DataSetTypeProto type = 3;	// 테이터세트 종류
	marmot.proto.RecordSchemaProto record_schema = 4; // 데이터세트 컬럼 정보
	oneof optional_geometry_info {	// 공간 컬럼 정보
		DataSetGeometryInfoProto geometry_info = 5;
	}
	int64 record_count = 6;		// 레코드 수
	string hdfs_path = 7;		// HDFS 저장 경로
	bool compression = 8;		// 압축 저장 여부
	int64 block_size = 9;		// HDFS 파일 블록 크기 (기본 64mb)
}

message DataSetInfoResponse {
	oneof either {
		DataSetInfoProto dataset_info = 1;
		marmot.proto.service.ErrorProto error = 2;
	}
}

message CreateDataSetRequest {
	string id = 1;						// 생성하고자 하는 데이터세트의 식별자.
	oneof either_initialization {
		marmot.proto.RecordSchemaProto record_schema = 2;	// 생성하고자 하는 데이터세트의 스키마.
		// 생성된 데이터세트에 채우기 위해 수행할 실행계획.
		// 생성될 데이터세트의 스키마 정보는 Plan 수행 결과 스키마를 사용한다.
		marmot.proto.service.ExecutePlanRequest plan_exec = 3;
	}
	oneof optional_options {
		DataSetOptionsProto options = 4;
	}
}

message AppendRecordSetRequest {  
	string id = 1;		// 업로드 대상 데이터세트 식별자
	string rset_id = 2;	// 업로드할 레코드세트 식별자
	oneof optional_plan {
		// 추가하는 레코드를 서버쪽에서 전처리할 필요한 경우 사용될 Plan
		marmot.proto.optor.PlanProto plan = 3;
	}
}
	
message BindDataSetRequest {
	string dataset = 1;
	string file_path = 2;
	DataSetTypeProto type = 3;
	oneof optional_geometry_info {
		GeometryColumnInfoProto geometry_info = 4;
	}
}

message ClusterDataSetRequest {
	string id = 1;		// 클러스터링 대상 데이터세트 식별자.
	oneof optional_quad_key_file {
		string quad_key_file = 2;
	}
	oneof optional_sample_ratio {
		double sample_ratio = 3;
	}
	oneof optional_block_size {
		int64 block_size = 4;	// 생성될 클러스터 파일의 블럭 크기.
	}
	oneof optiona_block_fill_ratio {
		double block_fill_ratio = 5;
	}
	oneof optional_worker_count {
		int32 worker_count = 6;	// 클러스터 생성 reducer 갯수
	}
}

message MoveDataSetRequest {
	string src_id = 1;
	string dest_id = 2;
}

message MoveDirRequest {
	string src_path = 1;
	string dest_path = 2;
}

message QueryRangeRequest {
	string id = 1;				// 검색 대상 데이터세트 식별자
	EnvelopeProto range = 2;	// 검색 대상 영역
	oneof optional_filter_epxr {
		string filter_expr = 3;	// 검색 조건
	}
}



message SpatialIndexInfoResponse {
	oneof either {
		marmot.proto.SpatialIndexInfoProto info = 1;
		marmot.proto.service.ErrorProto error = 2;
	}
}
message QuerySpatialClusterInfoRequest {
	string dataset_id = 1;		// 대상 데이터세트 식별
	EnvelopeProto bounds = 2;	// 검색 영역 (MBR)
}
message SpatialClusterInfoResponse {
	oneof either {
		SpatialClusterInfoProto spatial_cluster_info = 1;	// 공간 클러스터 등록정보
		ErrorProto error = 2;
	}
}
message SpatialClusterInfoProto {
	string quad_key = 1;			// 공간 클러스터 식별
	EnvelopeProto tile_bounds = 2;	// 공간 클러스터 대상 영역
	EnvelopeProto data_bounds = 3;	// 공간 클러스터에 저장된 데이터의 MBR
	int32 record_count = 4;			// 공간 클러스터에 저장된 레코드 수
	int64 byte_length = 5;			// 공간 클러스터 크기 (byte 단위)
}
message ReadSpatialClusterRequest {
	string dataset_id = 1;			// 접근하려는 데이터세트 식별자.
	string quad_key = 2;			// 읽으려는 공간 클러스터 (인덱스 블럭) 식별자.
	oneof optional_filter_epxr {	// 검색 조건 (optional)
		string filter_epxr = 3;
	}
}
message SampleSpatialClusterRequest {
	string dataset_id = 1;
	string quad_key = 2;
	EnvelopeProto bounds = 3;
	double sample_ratio = 4;
}

message DirectoryTraverseRequest {
	string directory = 1;	// 검색 대상 시작 디렉토리 경로
	bool recursive = 2;		// 하위 디렉토리 검색 여부.
}

message CreateKafkaTopicRequest {
	string topic = 1;
	bool force = 2;
}